{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: FAISS Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from face_alignment import align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"dataset/IMDb-Face_clean_unique.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load face embedding model\n",
    "model_face_embedding = torch.hub.load('otroshi/edgeface', 'edgeface_s_gamma_05', source='github', pretrained=True)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model_face_embedding = model_face_embedding.to(device)\n",
    "\n",
    "# Set model to eval\n",
    "model_face_embedding.eval()\n",
    "\n",
    "print(f\"Model is loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model\n",
    "model_yolo = YOLO(\"yolov11s-face.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in sample_df.iterrows():\n",
    "#     image_path = os.path.join(\"dataset\", \"images\", row[\"index\"], row[\"image\"])  # Construct the file path\n",
    "\n",
    "#     # Load the image with OpenCV\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     if image is not None:\n",
    "#         # Extract height and width from the row\n",
    "#         height, width = map(int, row[\"height width\"].split())  # Assuming height and width are stored as space-separated values\n",
    "\n",
    "#         # Resize the image based on the given height and width\n",
    "#         resized_image = cv2.resize(image, (width, height))\n",
    "\n",
    "#         # Run YOLO inference\n",
    "#         results = model_yolo(resized_image)  \n",
    "\n",
    "#         # Extract bounding boxes\n",
    "#         if len(results) > 0:\n",
    "#             boxes = results[0].boxes  # Get detected bounding boxes\n",
    "\n",
    "#             if len(boxes) > 0:\n",
    "#                 # Extract CSV face coordinates\n",
    "#                 x1_csv, y1_csv, x2_csv, y2_csv = map(int, row[\"rect\"].split())\n",
    "\n",
    "#                 # Compute CSV face center\n",
    "#                 cx_csv = (x1_csv + x2_csv) / 2\n",
    "#                 cy_csv = (y1_csv + y2_csv) / 2\n",
    "\n",
    "#                 closest_box = None\n",
    "#                 min_distance = float(\"inf\")\n",
    "\n",
    "#                 # Iterate over detected faces\n",
    "#                 for box in boxes:\n",
    "#                     x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "\n",
    "#                     # Compute center of detected face\n",
    "#                     cx_det = (x1 + x2) / 2\n",
    "#                     cy_det = (y1 + y2) / 2\n",
    "\n",
    "#                     # Compute Euclidean distance\n",
    "#                     distance = np.sqrt((cx_det - cx_csv) ** 2 + (cy_det - cy_csv) ** 2)\n",
    "\n",
    "#                     # Update the closest face\n",
    "#                     if distance < min_distance:\n",
    "#                         min_distance = distance\n",
    "#                         closest_box = (int(x1), int(y1), int(x2), int(y2))\n",
    "\n",
    "#                 # Crop the closest face\n",
    "#                 if closest_box:\n",
    "#                     x1, y1, x2, y2 = closest_box\n",
    "\n",
    "#                     margin = 30\n",
    "#                     h, w, _ = resized_image.shape  # Get image dimensions\n",
    "\n",
    "#                     # Clip coordinates to stay within image bounds\n",
    "#                     x1 = max(0, x1 - margin)\n",
    "#                     y1 = max(0, y1 - margin)\n",
    "#                     x2 = min(w, x2 + margin)\n",
    "#                     y2 = min(h, y2 + margin)\n",
    "\n",
    "#                     face_image = resized_image[y1:y2, x1:x2]\n",
    "\n",
    "#                     # Plot Image\n",
    "#                     image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "#                     plt.imshow(image_rgb)\n",
    "#                     plt.axis(\"off\")  # Hide axis\n",
    "#                     plt.show()\n",
    "\n",
    "#                     # Get Embedding\n",
    "#                     # Convert the OpenCV image (BGR) to PIL image (RGB)\n",
    "#                     pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "#                     aligned = align.get_aligned_face(None, pil_image) # align face\n",
    "\n",
    "#                     # Check If alignment result good\n",
    "#                     if aligned is not None:\n",
    "                        \n",
    "#                         # Plot Image\n",
    "#                         plt.imshow(aligned)\n",
    "#                         plt.axis(\"off\")  # Hide axis\n",
    "#                         plt.show()\n",
    "\n",
    "#                         transformed_input = transform(aligned).unsqueeze(0).to(device) # preprocessing\n",
    "\n",
    "#                         # extract embedding\n",
    "#                         face_embedding = model_face_embedding(transformed_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index (using L2 distance for simplicity)\n",
    "dimension = 512  # Adjust this based on the size of your embeddings (e.g., 128 or 512)\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # Using L2 distance\n",
    "\n",
    "# List to store image paths and corresponding embeddings\n",
    "image_paths = []\n",
    "embeddings_list = []\n",
    "\n",
    "# Process each image\n",
    "for index, row in tqdm(df.iterrows(), desc=\"Processing images\", total=len(df)):\n",
    "    image_path = os.path.join(\"dataset\", \"images\", row[\"index\"], row[\"image\"])\n",
    "\n",
    "    # Load the image with OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is not None:\n",
    "        # Get embedding for the image\n",
    "        x1, y1, x2, y2 = map(int, row[\"rect\"].split())  # Split 'rect' and convert to integers\n",
    "        height, width = map(int, row[\"height width\"].split())  # Assuming height and width are space-separated\n",
    "\n",
    "        resized_image = cv2.resize(image, (width, height))\n",
    "        cropped_image = resized_image[y1:y2, x1:x2]  # Crop the image\n",
    "\n",
    "        # Plot Image\n",
    "        plt.imshow(cropped_image)\n",
    "        plt.axis(\"off\")  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "        aligned = align.get_aligned_face(None, pil_image)  # align face\n",
    "\n",
    "        if aligned is not None:\n",
    "            transformed_input = transform(aligned).unsqueeze(0).to(device)  # preprocessing\n",
    "            face_embedding = model_face_embedding(transformed_input).cpu().detach().numpy().flatten()\n",
    "\n",
    "            # Add the embedding to the FAISS index\n",
    "            faiss_index.add(np.array([face_embedding]))  # Add the embedding to the FAISS index\n",
    "\n",
    "            # Save the image path and embedding for later retrieval\n",
    "            image_paths.append(image_path)\n",
    "            embeddings_list.append(face_embedding)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(faiss_index, 'face_embeddings.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index from disk\n",
    "faiss_index = faiss.read_index('face_embeddings.index')\n",
    "\n",
    "# Example: Query with a specific face embedding (e.g., first embedding in the list)\n",
    "# query_embedding = embeddings_list[0]  # Let's use the first image's embedding as the query\n",
    "image_path = \"face_3.png\"\n",
    "input_image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to RGB\n",
    "input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(input_image_rgb)\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "pil_image = Image.fromarray(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB))\n",
    "aligned = align.get_aligned_face(None, pil_image)  # align face\n",
    "\n",
    "if aligned is not None:\n",
    "    transformed_input = transform(aligned).unsqueeze(0).to(device)  # preprocessing\n",
    "    face_embedding = model_face_embedding(transformed_input).cpu().detach().numpy().flatten()\n",
    "else:\n",
    "    print(\"invalid alignment\")\n",
    "\n",
    "query_embedding = face_embedding\n",
    "\n",
    "# Perform the search for the top k nearest neighbors\n",
    "k = 5  # Number of nearest neighbors you want to retrieve\n",
    "D, I = faiss_index.search(np.array([query_embedding]), k)  # D is the distances, I is the indices of nearest neighbors\n",
    "\n",
    "# Output the results\n",
    "print(\"Indices of nearest neighbors:\", I)\n",
    "print(\"Distances to nearest neighbors:\", D)\n",
    "\n",
    "# Plot images of the nearest neighbors\n",
    "for idx in I[0]:\n",
    "    image_path = image_paths[idx]  # Get the image path of the nearest neighbor\n",
    "    image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "    \n",
    "    if image is not None:\n",
    "        # Convert the image to RGB\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Plot the image\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.axis(\"off\")  # Hide axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Failed to load image at {image_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
